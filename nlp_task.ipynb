{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorship Detection for Ukrainian Songs. Binary Classificaton on two artists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and preprocess the datasets. \n",
    "* remove apostrophes before tokenizing\n",
    "* use morphic analyser for lemmatization (Porter / Lancaster stemmer are not available for Ukrainian) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from math import log, floor\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer  # !! UA-version is included in the .git installation only.\n",
    "# + pip install pymorphy2-dicts-uk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer();  # nltk.tokenize.RegexpTokenizer(\"[.!?,-]{1,}|[а-яА-Яіїє]{1,}|[а-яА-Яіїє]{1,}[-’]?[а-яА-Яіїє]+)\"\n",
    "morphic_analyser = MorphAnalyzer(lang='uk')\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "def get_song_data(filename):\n",
    "    ''' Read the lyrics from file; return tokenized content. '''\n",
    "    with open(filename, 'r') as file:\n",
    "        raw_text = file.read()\n",
    "        raw_text = raw_text.replace('’', '')\n",
    "        raw_text = raw_text.replace(\"'\", '')\n",
    "        return tokenizer.tokenize(text=raw_text)\n",
    "    \n",
    "    \n",
    "def lemmatize(tokens_list):\n",
    "    ''' Find normal form of the tokens by morphological rules in pymorphy2. '''\n",
    "    \n",
    "    def is_word(token):\n",
    "        return len(list(filter(lambda x: x not in punctuation, token))) >= 1\n",
    "    \n",
    "    def get_most_likely_parse(word_analysis):\n",
    "        most_common_part_of_speech = Counter([x.tag._str[:4] for x in word_analysis]).most_common(1)[0][0]\n",
    "        parse = max([opt for opt in word_analysis if opt.tag._str[:4] == most_common_part_of_speech], key=lambda x: x.score)\n",
    "        return parse\n",
    "\n",
    "    return list(map(lambda x: get_most_likely_parse(morphic_analyser.parse(x)).normal_form, tokens_list))\n",
    "\n",
    "\n",
    "def get_num_grade(num, count_sys=10):\n",
    "    return floor(log(num, count_sys))\n",
    "\n",
    "\n",
    "def get_data(path, FILE_N_LEN, FILE_N_RANGE):\n",
    "    assert(floor(log(FILE_N_RANGE, 10)) <= FILE_N_LEN)\n",
    "\n",
    "    files_names = ['0' * (FILE_N_LEN - 1 - get_num_grade(file_n)) + str(file_n) \n",
    "                   for file_n in range(1, FILE_N_RANGE+1)]\n",
    "\n",
    "    data = [' '.join(lemmatize(get_song_data(path.format(file_name)))) for file_name in files_names]\n",
    "    return data\n",
    "\n",
    "\n",
    "def print_data(data):\n",
    "    for d in data:\n",
    "        print(d, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_data = get_data('./train/texts/{}.txt', 3, 175)\n",
    "test_data = get_data('./test/texts/{}.txt', 3, 42)\n",
    "all_data = train_data + test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract modeling features from the datasets. \n",
    "I use __Bag-of-words__ approach for the feature-extraction. Here a dataframe is being created from the whole x-data. Values in its every column (=word) are computed from the occurency of a word in this document in relation to its occurency in whole x-data. Such approach is called __Term Frequency-Inverse Document Frequency__ method (alternatives: binary encoding, simple word-counting) - it assumes that a less-frequently-occurring word is usually more important to the text than a typical one. Using Bag-of-Words method allows to balance out most of too typical words like articles, conjunctions, and pronouns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "vectorizer.fit(all_data)\n",
    "x_train = pd.DataFrame(vectorizer.transform(train_data).toarray(), columns=sorted(vectorizer.vocabulary_.keys()))\n",
    "y_train_raw = pd.read_json(\"./train/labels.json\")\n",
    "\n",
    "y_train = label_binarize(y_train_raw.values[0], classes=[\"Тартак\", \"Океан Ельзи\"]).ravel()\n",
    "\n",
    "print(y_train)\n",
    "# y_train = list([1 if x == \"Тартак\" else 0 for x in y_train_raw.values[0]])\n",
    "\n",
    "x_test = pd.DataFrame(vectorizer.transform(test_data).toarray(), columns=sorted(vectorizer.vocabulary_.keys()))\n",
    "y_test_raw = pd.read_json(\"./test/labels.json\")\n",
    "y_test = label_binarize(y_test_raw.values[0], classes=[\"Тартак\", \"Океан Ельзи\"]).ravel()\n",
    "\n",
    "# y_test = list([1 if x == \"Тартак\" else 0 for x in y_test_raw.values[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the accuracy and ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "\n",
    "def compute_roc_auc(y_test, y_pred):    \n",
    "    fpr, tpr, thrs = roc_curve(y_test, y_pred)\n",
    "    return auc(fpr, tpr)\n",
    "\n",
    "\n",
    "def compute_accuraccy(y_test, y_pred):\n",
    "    return 1 - len([x for i, x in enumerate(y_pred) if x != y_test[i]]) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \n",
      " 0.9047619047619048 \n",
      "\n",
      "AUC (Area Under Curve): \n",
      " 0.9 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy: \\n {} \\n\".format(compute_accuraccy(y_test, y_pred)))\n",
    "print(\"AUC (Area Under Curve): \\n {} \\n\".format(compute_roc_auc(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a classifier and make predictions on the training and the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \n",
      " 0.7857142857142857 \n",
      "\n",
      "AUC (Area Under Curve): \n",
      " 0.7857142857142857 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy: \\n {} \\n\".format(compute_accuraccy(y_test, y_pred)))\n",
    "print(\"AUC (Area Under Curve): \\n {} \\n\".format(compute_accuraccy(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \n",
      " 0.8571428571428572 \n",
      "\n",
      "AUC (Area Under Curve): \n",
      " 0.8571428571428572 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(\"Accuracy: \\n {} \\n\".format(compute_accuraccy(y_test, y_pred)))\n",
    "print(\"AUC (Area Under Curve): \\n {} \\n\".format(compute_accuraccy(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \n",
      " 0.7857142857142857 \n",
      "\n",
      "AUC (Area Under Curve): \n",
      " 0.7857142857142857 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(\"Accuracy: \\n {} \\n\".format(compute_accuraccy(y_test, y_pred)))\n",
    "print(\"AUC (Area Under Curve): \\n {} \\n\".format(compute_accuraccy(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
